{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1f780f6-d170-4e60-a0d3-c500bb5c8d76",
   "metadata": {},
   "source": [
    "## Extract keywords and separate tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a432c0-e62b-4ffd-96d5-a2f1aa5997b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from multi_rake import Rake\n",
    "import pandas as pd\n",
    "import time\n",
    "import lemmy\n",
    "\n",
    "os.chdir(\"/folder\")\n",
    "\n",
    "csv_input_file_path = './input.csv'\n",
    "csv_output_file_path_pre_lemmy_ddmmyy_to_ddmmyy = \"./output_temp.csv\"\n",
    "csv_input_file_path_pre_lemmy_ddmmyy_to_ddmmyy = csv_output_file_path_pre_lemmy_ddmmyy_to_ddmmyy \n",
    "csv_output_file_path_post_lemmy_ddmmyy_to_ddmmyy = \"./output.csv\" \n",
    "\n",
    "for file in [csv_input_file_path]:  \n",
    "    print(f\"Processing file: {file}\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    rake = Rake(\n",
    "        min_chars=3, \n",
    "        max_words=2,  \n",
    "        min_freq=1,  \n",
    "        language_code=None,\n",
    "        stopwords=None,  \n",
    "        lang_detect_threshold=50,  \n",
    "        max_words_unknown_lang=2,  \n",
    "        generated_stopwords_percentile=80,  \n",
    "        generated_stopwords_max_len=3,  \n",
    "        generated_stopwords_min_freq=2,  \n",
    "    )\n",
    "\n",
    "    df = pd.read_csv(csv_input_file_path, encoding=\"utf-8\", sep = \";\")\n",
    "    output_data = []\n",
    "    for _, row in df.iterrows():\n",
    "        article_id = row.iloc[0]  \n",
    "        article_text = str(row.iloc[6]) \n",
    "        try:\n",
    "            keywords = rake.apply(article_text)  \n",
    "        except Exception as e:\n",
    "            if \"invalid UTF-8\" in str(e):  \n",
    "                print(f\"Skipping row {article_id} due to UTF-8 encoding error: {e}\")\n",
    "                continue  \n",
    "            else:\n",
    "                raise  \n",
    "\n",
    "        top_keywords = [kw[0] for kw in keywords[:20]] \n",
    "        top_keywords = top_keywords + [\"\"] * (20 - len(top_keywords))\n",
    "        individual_words = set()\n",
    "        for kw in top_keywords:\n",
    "            if \" \" in kw: \n",
    "                words = kw.split()\n",
    "                individual_words.update(words)\n",
    "        individual_words = list(individual_words)[:40]  \n",
    "        individual_words += [\"\"] * (40 - len(individual_words))\n",
    "        output_data.append([article_id] + top_keywords + individual_words + [article_text])\n",
    "    keyword_columns = [f\"Keyword_{i+1}\" for i in range(20)]\n",
    "    word_columns = [f\"Word_{i+1}\" for i in range(40)]\n",
    "    output_df = pd.DataFrame(output_data, columns=[\"ID\"] + keyword_columns + word_columns + [\"Article_text\"])\n",
    "    output_df.to_csv(csv_output_file_path_pre_lemmy_ddmmyy_to_ddmmyy, mode='w', sep=\";\", index=False)\n",
    "    print(f\"Finalised extracting keywords in {df.shape[0]} out of {df.shape[0]} articles\")\n",
    "\n",
    "    lemmatizer = lemmy.load(\"da\")\n",
    "\n",
    "    df = pd.read_csv(csv_input_file_path_pre_lemmy_ddmmyy_to_ddmmyy, delimiter=\";\", dtype=str)\n",
    "    def lemmatize_phrase(phrase):\n",
    "        if pd.notna(phrase):\n",
    "            words = phrase.strip('\"').split() \n",
    "            lemmatized_words = [lemmatizer.lemmatize(\"\", word)[0] for word in words]  \n",
    "            return \" \".join(lemmatized_words) \n",
    "        return phrase\n",
    "    for col in df.columns[1:]:  \n",
    "        if col != \"Article_text\":  \n",
    "            df[col] = df[col].apply(lemmatize_phrase)\n",
    "    df.to_csv(csv_output_file_path_post_lemmy_ddmmyy_to_ddmmyy, sep=\";\", index=False)\n",
    "    print(f\"Finalised lemmatizing {df.shape[0]} out of {df.shape[0]} rows\")\n",
    "\n",
    "    end_time = time.time() \n",
    "    runtime = end_time - start_time  \n",
    "\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f0e2a4-c4b3-4cb3-a628-bd73eba97da9",
   "metadata": {},
   "source": [
    "## Check for target words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae3bf4d-30fd-4fc7-99ad-90d634180d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "targets = {\n",
    "    \"rusland\": None,\n",
    "    \"putin\": None,\n",
    "    \"kina\": None,\n",
    "    \"jinping\": None,\n",
    "    \"klimaforandring\": None,\n",
    "    \"klima\": None,\n",
    "    \"indvandrer\": None,\n",
    "    \"indvandring\": None,\n",
    "    \"udlænding\": None,\n",
    "    \"asylansøger\": None,\n",
    "    \"muslim\": None,\n",
    "    \"islam\": None,\n",
    "    \"mette frederiksen\": None,\n",
    "    \"lars løkke\": None,\n",
    "    \"statsminister\": None,\n",
    "    \"enhedslisten\": None,\n",
    "    \"sf\": None,\n",
    "    \"radikale venstre\": None,\n",
    "    \"socialdemo\": None,\n",
    "    \"venstre\": None,\n",
    "    \"konservativ\": None,\n",
    "    \"liberal alliance\": None,\n",
    "    \"dansk folkeparti\": None,\n",
    "    \"danmarksdemokrat\": None,\n",
    "    \"moderaterne\": None, \n",
    "    \"ukrain\" : None,\n",
    "}\n",
    "\n",
    "os.chdir('/folder')\n",
    "input_file_path = './input.csv'  \n",
    "output_file_path = './output.csv'    \n",
    "\n",
    "print(f\"Processing file: {input_file_path}\")\n",
    "start_time = time.time()\n",
    "\n",
    "def target_in_text(text):\n",
    "    check_ = {}\n",
    "    text_lower = text.lower()\n",
    "    text_clean = re.sub(r\"[^\\w\\s]\", \" \", text_lower)\n",
    "\n",
    "    tokens = text_clean.split()\n",
    "\n",
    "    for target in targets:\n",
    "        if target == \"venstre\":\n",
    "            match = 0\n",
    "            for i, token in enumerate(tokens):\n",
    "                if token == \"venstre\":\n",
    "                    if i == 0 or tokens[i - 1] != \"radikale\":\n",
    "                        match = 1\n",
    "                        break\n",
    "            check_[target] = match\n",
    "\n",
    "        elif target == \"sf\":\n",
    "            pattern = r'\\bsf\\b'\n",
    "            check_[target] = 1 if re.search(pattern, text_clean) else 0\n",
    "\n",
    "        else:\n",
    "            check_[target] = 1 if target in text_clean else 0\n",
    "\n",
    "    return check_\n",
    "\n",
    "df = pd.read_csv(input_file_path, sep=\";\", quotechar='\"')\n",
    "output_data = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    article_id = row.iloc[0]\n",
    "    article_text = str(row.iloc[6])\n",
    "    check_results = target_in_text(article_text)\n",
    "    output_data.append([article_id] + list(check_results.values()))\n",
    "\n",
    "    if idx % 100 == 0: \n",
    "        print(f\"Processed {idx+1} / {df.shape[0]} rows\")\n",
    "\n",
    "output_df = pd.DataFrame(output_data, columns=[\"ID\"] + [f\"check_{t}\" for t in targets])\n",
    "output_df.to_csv(output_file_path, sep=\";\", index=False)\n",
    "\n",
    "end_time = time.time()\n",
    "runtime = end_time - start_time\n",
    "print(f\"Runtime: {runtime:.2f} seconds\")\n",
    "print(f\"Output saved to {output_file_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5737cab-cae0-4f4b-ad3e-549c212e9498",
   "metadata": {},
   "source": [
    "## sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46666d5f-a255-41af-8826-e5e4e7ed7e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from sentida import Sentida\n",
    "\n",
    "os.chdir(\"/folder\")\n",
    "input_file_path = './input.csv' \n",
    "output_file_path = './output.csv' \n",
    "\n",
    "print(f\"Processing file: {input_file_path}\")\n",
    "start_time = time.time()\n",
    "\n",
    "SV = Sentida()\n",
    "\n",
    "def sentiment_of_text(article_text): \n",
    "    if not isinstance(article_text, str) or article_text.strip() == \"\" or pd.isna(article_text):\n",
    "        return 0  \n",
    "\n",
    "    try:\n",
    "        score = SV.sentida(text=article_text, output=\"mean\", normal=True)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing text: {article_text[:50]}... - {e}\")\n",
    "        return 0  \n",
    "\n",
    "    return score\n",
    "\n",
    "df = pd.read_csv(input_file_path, sep=\",\")\n",
    "output_data = []\n",
    "\n",
    "with open(output_file_path, 'w') as output_file:\n",
    "    output_file.write(\"ID;Score\\n\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        article_id = row.iloc[0]\n",
    "        article_text = str(row.iloc[6]) \n",
    "        article_sentiment = sentiment_of_text(article_text)\n",
    "\n",
    "        output_file.write(f\"{article_id};{article_sentiment}\\n\")\n",
    "        \n",
    "        if idx % 100 == 0 and idx > 0:\n",
    "            elapsed_time = time.time() - start_time\n",
    "            time_per_row = elapsed_time / (idx + 1)\n",
    "            print(f\"Processed {idx + 1} rows in {elapsed_time:.2f} seconds. Time per row: {time_per_row:.4f} seconds\")\n",
    "\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Total time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Output saved to {output_file_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
