{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb3d416d-64d2-4a1c-ad21-044feedb644a",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2702dbf-c439-4595-ba6d-17ba7c9f0198",
   "metadata": {},
   "source": [
    "## Scraping the Wayback machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0aa47d-9fe8-4cde-8c7e-172860e78600",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "\n",
    "dataLoc = Path(\"/folder\") \n",
    "output_file = dataLoc / \"output.csv\"\n",
    "\n",
    "end_date = datetime(2015, 1, 1) \n",
    "start_date = datetime(2024, 12, 31) \n",
    "\n",
    "sources = [\"dr\", \"tv2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ecdef7-57a0-4a71-806b-1aa547d261b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(output_file, mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    \n",
    "    writer.writerow(['Index', 'Source', 'Date', 'Archive_URL', 'ID'])\n",
    "    \n",
    "    current_date = start_date\n",
    "    i = 1\n",
    "    \n",
    "    while current_date >= end_date:\n",
    "        formatted_date = current_date.strftime('%Y%m%d')\n",
    "        for source in sources:\n",
    "            archive_url = f\"https://web.archive.org/web/{formatted_date}120000/https://www.{source}.dk/\"\n",
    "            id_value = f\"{source}_{formatted_date}_{str(i).zfill(5)}\"\n",
    "            writer.writerow([i, source, formatted_date, archive_url, id_value])\n",
    "            i += 1\n",
    "        current_date -= timedelta(days=1)\n",
    "\n",
    "print(f\"CSV file saved successfully at '{output_file}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9879f8e6-6281-4e9b-96f2-20c53724fdf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "dataLoc = Path(\"/folder\") \n",
    "csv_file_path = dataLoc / \"output.csv\"\n",
    "csv_output_folder = dataLoc / \"folder\"\n",
    "\n",
    "def is_article_link(url):\n",
    "    if 'tv2.dk' in url:\n",
    "        if '/reel/' in url: \n",
    "            return False\n",
    "        if '/live/' in url: \n",
    "            return False\n",
    "        if 'nyheder.' not in url and 'nyhederne.' not in url and 'politik.' not in url and 'Nyheder.' not in url and 'Nyhederne.' not in url and 'Politik.' not in url and 'finans.' not in url and 'Finans' not in url and 'vejret' not in url and 'Vejret' not in url:\n",
    "            return False\n",
    "        last_part = url.split('/')[-1]\n",
    "        return '-' in last_part\n",
    "   \n",
    "    if 'dr.dk' in url:\n",
    "        if '/reel/' in url: \n",
    "            return False\n",
    "        if '/seneste/' in url: \n",
    "            return False\n",
    "        if '/ultra/' in url: \n",
    "            return False\n",
    "        if '/p3/' in url: \n",
    "            return False\n",
    "        if '/tv-guide/' in url: \n",
    "            return False\n",
    "        if '/etik-og-rettelser/' in url: \n",
    "            return False\n",
    "        if '/det-bedste-fra-dr/' in url: \n",
    "            return False\n",
    "        if '/nyheder/' not in url and 'Nyheder' not in url: \n",
    "            return False\n",
    "        if '/om-dr/' in url: \n",
    "            return False\n",
    "            \n",
    "        last_part = url.split('/')[-1]\n",
    "        return '-' in last_part\n",
    "    \n",
    "    return False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac4fdba-d257-458d-9b8e-cb3d1f224860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_links(base_url):\n",
    "    try:\n",
    "        response = requests.get(base_url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        all_links = [\n",
    "            link['href'] if link['href'].startswith('http') else base_url + link['href']\n",
    "            for link in soup.find_all('a', href=True)\n",
    "        ]\n",
    "        \n",
    "        article_links = list(set(filter(is_article_link, all_links)))\n",
    "        return article_links\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing {base_url}: {e}\")\n",
    "        return []\n",
    "      \n",
    "def clean_url(url):\n",
    "    wayback_prefix = \"web.archive.org/web/\"\n",
    "    if wayback_prefix in url:\n",
    "        url = url.split(wayback_prefix, 1)[-1]\n",
    "        if \"/\" in url:\n",
    "            url = url.split(\"/\", 1)[-1]\n",
    "\n",
    "    if \"www.dr.dk\" in url:\n",
    "        url = re.sub(r'//web/\\d+/https?://', '', url)\n",
    "        url = re.sub(r'^https:/+', '', url)\n",
    "        url = re.sub(r'www\\.dr\\.dkwww\\.dr\\.dk', 'www.dr.dk', url)\n",
    "\n",
    "    url = re.sub(r'^https?://', '', url)\n",
    "\n",
    "    return url\n",
    "    \n",
    "def process_links(csv_file_path, csv_output_folder):\n",
    "    with open(csv_file_path, mode='r', encoding='utf-8') as infile:\n",
    "        reader = csv.reader(infile, delimiter=',')\n",
    "        urls = list(reader)\n",
    "    \n",
    "    if not os.path.exists(csv_output_folder):\n",
    "        os.makedirs(csv_output_folder)\n",
    "    \n",
    "    for row in urls:\n",
    "        if row: \n",
    "\n",
    "            full_url = row[3] \n",
    "            base_url = full_url.split(\"https://\", 1)[-1]  \n",
    "            base_url = \"https://\" + base_url \n",
    "            unique_id = row[4] \n",
    "            print(f\"Scraping links from: {base_url} (ID: {unique_id})\")\n",
    "            \n",
    "            time.sleep(13)\n",
    "            \n",
    "            scraped_links = scrape_links(base_url) \n",
    "            \n",
    "            if not scraped_links: \n",
    "                print(f\"Skipping ID {unique_id} due to error or no links found.\")\n",
    "                continue \n",
    "\n",
    "            cleaned_links = [clean_url(link) for link in scraped_links]\n",
    "            cleaned_links = list(set(cleaned_links))\n",
    "\n",
    "            output_csv = f\"{unique_id}.csv\" \n",
    "            output_path = os.path.join(csv_output_folder, output_csv)\n",
    "            \n",
    "            with open(output_path, mode='w', encoding='utf-8', newline='') as outfile:\n",
    "                writer = csv.writer(outfile, delimiter=',')\n",
    "                for link in cleaned_links:\n",
    "                    writer.writerow([link])\n",
    "            \n",
    "            print(f\"Scraped links for ID {unique_id} saved to {output_path}\")\n",
    "\n",
    "process_links(csv_file_path, csv_output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09bf3970-a5ef-4b7a-afe6-e2aef170a02a",
   "metadata": {},
   "source": [
    "## Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d992ff0-2cd2-4456-b5a7-973e4555f34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "\n",
    "os.chdir('/folder') \n",
    "duplicates_folder = './'\n",
    "DUPLICATES_FILE = './output.csv'\n",
    "folder_path = './folder'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e0d2ef-0ef3-4df1-9999-6821d29e1b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_from_filename(filename):\n",
    "    match = re.search(r'(\\d{8})', filename) \n",
    "    if match:\n",
    "        return datetime.strptime(match.group(1), \"%Y%m%d\")\n",
    "    return None\n",
    "\n",
    "def get_recent_csv_files(folder_path, current_file):\n",
    "    all_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')], reverse=True)\n",
    "    \n",
    "    file_dates = {f: extract_date_from_filename(f) for f in all_files}\n",
    "    file_dates = {f: d for f, d in file_dates.items() if d}  \n",
    "    \n",
    "    if current_file not in file_dates:\n",
    "        return []\n",
    "    \n",
    "    current_date = file_dates[current_file]\n",
    "    return [f for f, d in file_dates.items() if current_date - timedelta(days=4) <= d < current_date]\n",
    "\n",
    "def normalize_url(url):\n",
    "    return url.strip().rstrip('/')  \n",
    "\n",
    "def remove_duplicates(folder_path, current_file):\n",
    "    recent_files = get_recent_csv_files(folder_path, current_file)\n",
    "    all_links = {}\n",
    "\n",
    "    print(f\"\\nProcessing {current_file} - Checking against: {recent_files}\")\n",
    "\n",
    "    for file in recent_files:\n",
    "        file_path = os.path.join(folder_path, file)\n",
    "        \n",
    "        if os.path.getsize(file_path) == 0:\n",
    "            print(f\"⚠️ Skipping empty file: {file}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file_path, header=None, encoding='utf-8', skip_blank_lines=True, on_bad_lines='skip')\n",
    "            if df.empty:\n",
    "                print(f\"⚠️ Skipping empty or invalid file: {file}\")\n",
    "                continue\n",
    "        except pd.errors.EmptyDataError:\n",
    "            print(f\"⚠️ Skipping corrupt or empty file: {file}\")\n",
    "            continue\n",
    "        \n",
    "        for link in df.iloc[:, 0].dropna().astype(str).apply(normalize_url):\n",
    "            all_links[link] = file  \n",
    "\n",
    "    current_path = os.path.join(folder_path, current_file)\n",
    "\n",
    "    if os.path.getsize(current_path) == 0:\n",
    "        print(f\"⚠️ Skipping empty file: {current_file}\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        df_current = pd.read_csv(current_path, header=None, encoding='utf-8', skip_blank_lines=True, on_bad_lines='skip')\n",
    "        if df_current.empty:\n",
    "            print(f\"⚠️ No valid data in {current_file}. Skipping.\")\n",
    "            return\n",
    "    except pd.errors.EmptyDataError:\n",
    "        print(f\"⚠️ Skipping corrupt or empty file: {current_file}\")\n",
    "        return\n",
    "\n",
    "    df_current['normalized_link'] = df_current.iloc[:, 0].astype(str).apply(normalize_url)\n",
    "    \n",
    "    duplicates = df_current[df_current['normalized_link'].isin(all_links.keys())]\n",
    "    df_current_filtered = df_current[~df_current['normalized_link'].isin(all_links.keys())].drop(columns=['normalized_link'])\n",
    "\n",
    "    df_current_filtered.to_csv(current_path, index=False, header=False, encoding='utf-8')\n",
    "\n",
    "    if not duplicates.empty:\n",
    "        duplicates = duplicates.copy()\n",
    "        duplicates.loc[:, 'source_file'] = duplicates['normalized_link'].map(all_links)\n",
    "        duplicates = duplicates[[0, 'source_file']]\n",
    "        duplicates.insert(0, 'removed_from', current_file)\n",
    "        \n",
    "        os.makedirs(duplicates_folder, exist_ok=True)\n",
    "        duplicates_file_path = os.path.join(duplicates_folder, DUPLICATES_FILE)\n",
    "        duplicates.to_csv(duplicates_file_path, index=False, mode='a', header=not os.path.exists(duplicates_file_path), encoding='utf-8')\n",
    "\n",
    "    print(f\"✅ Processed {current_file}: Removed {len(duplicates)} duplicates.\")\n",
    "\n",
    "def process_all_files(folder_path):\n",
    "    all_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.csv')], reverse=True)\n",
    "\n",
    "    for file in all_files:\n",
    "        remove_duplicates(folder_path, file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_all_files(folder_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1b1c3-a692-409f-a2c0-2f5f91e11ee4",
   "metadata": {},
   "source": [
    "## Scrape media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a2ed01-219c-4534-83e1-18e1960e75a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import os\n",
    "import shutil \n",
    "import random\n",
    "from PIL import Image\n",
    "import io\n",
    "from pathlib import Path\n",
    "\n",
    "dataLoc = Path('/folder')\n",
    "image_directory = dataLoc / \"image_folder/\"\n",
    "input_folder = dataLoc / \"folder\"\n",
    "output_csv = dataLoc / \"output.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c40c43-b36c-40f6-b67c-454abc9591fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def scrape_articles_from_folder(input_folder, output_csv, image_directory):\n",
    "    if os.path.exists(image_directory):\n",
    "        shutil.rmtree(image_directory) \n",
    "    os.makedirs(image_directory)\n",
    "    \n",
    "    with open(output_csv, mode='w', newline='', encoding='utf-8') as output_file:\n",
    "        writer = csv.writer(output_file)\n",
    "        writer.writerow([\"ID\", \"Title\", \"Theme\", \"Date\", \"Link\", \"Image ID\", \"Text\"])\n",
    "      \n",
    "        for input_csv in sorted(os.listdir(input_folder)):\n",
    "            if input_csv.endswith(\".csv\"):  \n",
    "                input_csv_path = os.path.join(input_folder, input_csv)\n",
    "                print(f\"Processing file: {input_csv_path}\")\n",
    "\n",
    "                count = 1\n",
    "                \n",
    "                with open(input_csv_path, mode='r', encoding='utf-8') as input_file:\n",
    "                    reader = csv.reader(input_file)\n",
    "                    links = [row[0] for row in reader]  \n",
    "                \n",
    "                for url in links:\n",
    "                    try:\n",
    "                        print(f\"Scraping: {url}\")\n",
    "\n",
    "                        response = requests.get(\"https://\" + url)\n",
    "                        response.raise_for_status()\n",
    "                        \n",
    "                        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "                        \n",
    "                        title = soup.find('h1').get_text(strip=True) if soup.find('h1') else \"No title found\"\n",
    "                        \n",
    "                        article_body = soup.find('div', class_='dre-article-body__content')\n",
    "                        if article_body:\n",
    "                            paragraphs = article_body.find_all('p')\n",
    "                            article_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs)\n",
    "                        else:\n",
    "                            paragraphs = soup.find_all('p') \n",
    "                            article_text = \"\\n\".join(p.get_text(strip=True) for p in paragraphs) if paragraphs else \"No text found\"\n",
    "\n",
    "                        if not article_text: \n",
    "                            continue\n",
    "\n",
    "                        ID = f\"{input_csv[:-4]}_{str(count).zfill(4)}\"\n",
    "                        \n",
    "                        \n",
    "                        print(ID)\n",
    "                        lead_image_tag = soup.find('meta', property='og:image')\n",
    "        \n",
    "                        if lead_image_tag and 'content' in lead_image_tag.attrs:\n",
    "                            image_url = lead_image_tag['content']  \n",
    "                            if image_url.lower().endswith('.gif'):\n",
    "                                print(f\"Skipping GIF: {image_url}\")\n",
    "                                image_url = None\n",
    "                                image_id = None\n",
    "                            else: \n",
    "                                image_id = ID + \".jpeg\"\n",
    "\n",
    "                        if not image_id: \n",
    "                            continue\n",
    "\n",
    "                        if image_url:\n",
    "                            try:\n",
    "                                response = requests.get(image_url, stream=True)\n",
    "                                response.raise_for_status()\n",
    "                                \n",
    "                                img = Image.open(io.BytesIO(response.content))\n",
    "                                img_format = img.format \n",
    "                                \n",
    "                                img.thumbnail((1280, 720), Image.Resampling.LANCZOS)\n",
    "                                \n",
    "                                image_path = os.path.join(image_directory, f\"{image_id}\")\n",
    "                                \n",
    "                                img_bytes = io.BytesIO()\n",
    "                                img.save(img_bytes, format='JPEG', quality=10, optimize=False)\n",
    "                                \n",
    "                                with open(image_path, 'wb') as img_file:\n",
    "                                    img_file.write(img_bytes.getvalue())\n",
    "                                \n",
    "                                print(f\"Image downloaded, resized, and saved as {image_path}\")\n",
    "                            \n",
    "                            except Exception as e:\n",
    "                                print(f\"Failed to download the image: {e}\")\n",
    "                                image_id = \"Image download failed\"\n",
    "\n",
    "                        theme = \"\"\n",
    "                        if \"dr\" in ID:\n",
    "                            if \"nyheder/\" in url:\n",
    "                                theme = url.split(\"nyheder/\")[1].split(\"/\")[0].lower()\n",
    "                                if theme not in {\"naturvidenskab\", \"miljoe\",\"kroppen\", \"tech\", \"teknologi\", \"klima\", \"detektor\", \"politik\", \"udland\", \"vejret\", \"indland\", \"viden\", \"penge\", \"kultur\", \"webfeature\", \"regionale\"}: \n",
    "                                    theme = \"\"\n",
    "                        elif \"tv2\" in ID:\n",
    "                            if \"nyheder.tv2.dk/\" in url:\n",
    "                                theme = url.split(\"nyheder.tv2.dk/\")[1].split(\"/\")[0].lower()\n",
    "                                if theme not in {\"klima\", \"finans\", \"krimi\", \"udland\", \"samfund\", \"business\", \"politik\", \"penge\", \"trafik\", \"tech\", \"erhverv\", \"lokalt\"}: \n",
    "                                    theme = \"\"\n",
    "                        \n",
    "                        if image_url:\n",
    "                            raw_date = input_csv.split('_')[1]  \n",
    "                            date = f\"{raw_date[:4]}-{raw_date[4:6]}-{raw_date[6:8]}\"  \n",
    "                            \n",
    "                            writer.writerow([image_id, title, theme, date, url, image_id, article_text])\n",
    "                        else:\n",
    "                            print(\"Skipping this article as no image was found.\")\n",
    "\n",
    "                        count += 1\n",
    "                    \n",
    "                    except Exception as e:\n",
    "                        print(f\"Failed to process the URL {url}: {e}\")\n",
    "\n",
    "scrape_articles_from_folder(input_folder, output_csv, image_directory)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
